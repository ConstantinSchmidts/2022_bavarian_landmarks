{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wc92cWK-Aas"
      },
      "source": [
        "# Finding landmarks in Videos\n",
        "\n",
        "\n",
        "In this notebook, you can extract the names and frames of landmarks from a video. \n",
        "The video is first analyzed with the [OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit) model (an open-vocabulary object detection model) by Google Research. \n",
        "We use [OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit) as a building detector by splitting the video into frames and querying each frame  for hand-crafted text templates descriptions of buildings (e.g., \"castle at daylight in full view\"). The time code of each frame in which this process detects something is saved and compared to the output of the [LandmarkNER](https://huggingface.co/spaces/constantinSch/LandmarkNER_EL).\n",
        "For the [LandmarkNER](https://huggingface.co/spaces/constantinSch/LandmarkNER_EL), German video subtitles are analyzed with a Named Entity Recognition for landmark trained on BR subtitle to recognize the names of buildings that occur in the subtitles. The output of the LandmarkNER is disambiguated for Wikipedia titles by [mGenre](https://github.com/facebookresearch/GENRE). \n",
        "\n",
        "The frames for which the OWL-VIT model detects a building and the [LandmarkNER](https://huggingface.co/spaces/constantinSch/LandmarkNER_EL) detects a landmark are saved and downloaded in a .zip file.\n",
        "\n",
        "The video needs to be uploaded as a .mp4 and the associated subtitles as a .txt with the timecodes in milliseconds. The name of the subtitle file has to start with the video-ID and can be followed by more information after an `_` ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1enDXxJYS4XM"
      },
      "source": [
        "# Building detection with [OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIcaig48T6yv"
      },
      "source": [
        "## Set-up environment for the building detector\n",
        "\n",
        "Install huggingface transformers version that includes OWL-VIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XLma_DL3S9-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPrCVnimE0qR"
      },
      "source": [
        "### Load pre-trained model and processor\n",
        "\n",
        "Let's first apply the image preprocessing and tokenize the text queries using `OwlViTProcessor`. The processor will resize the image(s), scale it between [0-1] range and normalize it across the channels using the mean and standard deviation specified in the original codebase.\n",
        "\n",
        "\n",
        "Text queries are tokenized using a CLIP tokenizer and stacked to output tensors of shape [batch_size * num_max_text_queries, sequence_length]. If you are inputting more than one set of (image, text prompt/s), num_max_text_queries is the maximum number of text queries per image across the batch. Input samples with fewer text queries are padded. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD8DXCnJ7faH"
      },
      "outputs": [],
      "source": [
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "\n",
        "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCryM3eFl_2j"
      },
      "source": [
        "## Preprocess input video\n",
        "To analyse an .mp4 video in OWL-Vit you first have to upload it in the Colab. The following function extracts every N-th frame of the video and converts it to the necessary format. \n",
        "You can change how many frames you want to extract bei adjusting N. Currently, we extract 1 frame every 1.5 seconds, assuming a frame rate of 50 frames per second. \n",
        "This makes the frames sufficiently different. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPwOdcrLnHa_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import glob\n",
        "\n",
        "# For videos directly uploaded to the Colab\n",
        "path = '/content/*.mp4' \n",
        "video_paths=glob.glob(path) \n",
        "\n",
        "video_files=[]\n",
        "video_files.append(video_paths[0])\n",
        "\n",
        "# How many frames to skip\n",
        "N = 75\n",
        "\n",
        "# The frame images will be stored in video_frames\n",
        "video_frames = []\n",
        "\n",
        "# The time codes in milliseconds will be stored in time_codes\n",
        "video_time_codes = []\n",
        "\n",
        "# Open the video files\n",
        "for file in video_files:\n",
        "  capture = cv2.VideoCapture(file)\n",
        "  fps = capture.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "  current_frame = 0\n",
        "  while capture.isOpened():\n",
        "  # Read the current frame\n",
        "    ret, frame = capture.read()\n",
        "\n",
        "  # Convert it to a PIL image (required for CLIP) and store it\n",
        "    if ret == True:\n",
        "      video_frames.append(Image.fromarray(frame[:, :, ::-1]))\n",
        "      video_time_codes.append(capture.get(cv2.CAP_PROP_POS_MSEC))\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  # Skip N frames\n",
        "    current_frame += N\n",
        "    capture.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
        "\n",
        "# Print some statistics\n",
        "print(f\"Frames extracted: {len(video_frames)}\")\n",
        "print(f\"Time codes extracted: {len(video_time_codes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HBYwadqLiKU"
      },
      "outputs": [],
      "source": [
        "# Look at the first extracted frame\n",
        "video_frames[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zryaaRCxQQEA"
      },
      "outputs": [],
      "source": [
        "# Look at the first 19 time codes\n",
        "video_time_codes[:19]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QN-vURe3euV"
      },
      "source": [
        "## Analyze each frame with OWL-ViT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVSHItVzL82V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Use GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qt0SUCnD9eS"
      },
      "outputs": [],
      "source": [
        "# Preprocessing all images \n",
        "import numpy as np\n",
        "\n",
        "# Preprocessing\n",
        "images = video_frames\n",
        "images = [Image.fromarray(np.uint8(img)).convert(\"RGB\") for img in images]\n",
        "\n",
        "# list of building types to detect\n",
        "text_queries = [\"palace at daylight in full view\", \"castle at daylight in full view\", \"skyscraper at daylight in full view\", \"parliament building at daylight in full view\", \"administrative building at daylight in full view\", \"municipal building at daylight in full view\", \"Corporate headquarters building at daylight in full view\", \"railway station building at daylight in full view\", \"exterior view of church building in daylight\", \"exterior view of mosque  building in daylight\", \"exterior view of synagoge building in daylight\"] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg4xoQCfFHM5"
      },
      "outputs": [],
      "source": [
        "# Set model in evaluation mode\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "outputs = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoAWyMmDEooe"
      },
      "outputs": [],
      "source": [
        "# Loop through images\n",
        "with torch.no_grad():\n",
        "  for image in images:\n",
        "    input = processor(text=text_queries, images=image, return_tensors=\"pt\").to(device)\n",
        "    # Get predictions\n",
        "    output = model(**input)\n",
        "    outputs.append(output)\n",
        "    # delete input and output to make room on the GPU\n",
        "    del(input)\n",
        "    del(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAbIlIiXb_N0"
      },
      "outputs": [],
      "source": [
        "len(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyFgHKUmR9zA"
      },
      "source": [
        "## Display frames with bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nrIn-xAR9zZ"
      },
      "outputs": [],
      "source": [
        "# Threshold to eliminate low probability predictions\n",
        "score_threshold = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvu39gikR9zZ"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(input_image, text_queries, scores, boxes, labels):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
        "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    for score, box, label in zip(scores, boxes, labels):\n",
        "      if score < score_threshold:\n",
        "        continue\n",
        "\n",
        "      cx, cy, w, h = box\n",
        "      ax.plot([cx-w/2, cx+w/2, cx+w/2, cx-w/2, cx-w/2],\n",
        "              [cy-h/2, cy-h/2, cy+h/2, cy+h/2, cy-h/2], \"r\")\n",
        "      ax.text(\n",
        "          cx - w / 2,\n",
        "          cy + h / 2 + 0.015,\n",
        "          f\"{text_queries[label]}: {score:1.2f}\",\n",
        "          ha=\"left\",\n",
        "          va=\"top\",\n",
        "          color=\"red\",\n",
        "          bbox={\n",
        "              \"facecolor\": \"white\",\n",
        "              \"edgecolor\": \"red\",\n",
        "              \"boxstyle\": \"square,pad=.3\"\n",
        "          })\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPhH5UTkR9za"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers.image_utils import ImageFeatureExtractionMixin\n",
        "mixin = ImageFeatureExtractionMixin()\n",
        "\n",
        "# Let's plot the predictions of the first 100 frames\n",
        "for i in range(100):\n",
        "  image_idx = i\n",
        "  image_size = model.config.vision_config.image_size\n",
        "  image = mixin.resize(images[image_idx], image_size)\n",
        "  input_image = np.asarray(image).astype(np.float32) / 255.0\n",
        "  # Get prediction logits\n",
        "  logits = torch.max(outputs[i][\"logits\"][0], dim=-1)\n",
        "  scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
        "  # Get prediction labels and boundary boxes\n",
        "  labels = logits.indices.cpu().detach().numpy()\n",
        "  boxes = outputs[i][\"pred_boxes\"][0].cpu().detach().numpy()\n",
        "  plot_predictions(image, text_queries, scores, boxes, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqTxUGn2UYel"
      },
      "source": [
        "## Detected Frames\n",
        "Collect the frames for which the model detected a building, display them and extract their timecodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ7L982KW8VF"
      },
      "outputs": [],
      "source": [
        "# Threshold to eliminate low probability predictions\n",
        "score_threshold = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ6bmSWRUYem"
      },
      "outputs": [],
      "source": [
        "# The index of the image building_frames\n",
        "building_frames_index = []\n",
        "\n",
        "# Loop over predictions for each image in the batch\n",
        "for i in range(len(video_frames)):\n",
        "  # Get prediction logits\n",
        "  logits = torch.max(outputs[i][\"logits\"][0], dim=-1)\n",
        "  scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
        "  # score_threshold = 0.2\n",
        "  for score in scores:\n",
        "     if score >= score_threshold:\n",
        "       building_frames_index.append(i)    \n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv9GPwsIUYem"
      },
      "outputs": [],
      "source": [
        "building_frames = []\n",
        "\n",
        "for frame in building_frames_index:\n",
        "  building_frames.append(video_frames[frame])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr3PcT89UYem"
      },
      "source": [
        "Display all frames in which the building detector detected anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgPmDbKqUYem"
      },
      "outputs": [],
      "source": [
        "# Look at all the frames in which buildings are detected\n",
        "for frame in building_frames:\n",
        "  display(frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8By14H41UYem"
      },
      "source": [
        "Create new lists with all frames time codes at which there were buildings detected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipZLBXIlUYem"
      },
      "outputs": [],
      "source": [
        "building_video_time_codes = []\n",
        "\n",
        "for frame in building_frames_index:\n",
        "  building_video_time_codes.append(video_time_codes[frame])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZqOUBwZRQzi"
      },
      "source": [
        "# Landmark detection in subtitles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnW9xUhCTHd4"
      },
      "source": [
        "## Set up environment to detect landmarks in subtitles\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkGu6asPCX8h"
      },
      "outputs": [],
      "source": [
        "# Install the landmarkNER model from huggingface\n",
        "!pip install https://huggingface.co/constantinSch/LandmarkNER/resolve/main/de_pipeline-any-py3-none-any.whl\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"de_pipeline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pkyp9IOoX-A"
      },
      "outputs": [],
      "source": [
        "# Download large German model to use its sentencizer component\n",
        "!python -m spacy download de_core_news_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdjvZMjg23jV"
      },
      "source": [
        "Install all necessary libraries and download mGENRE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SOxWsEetCHH"
      },
      "outputs": [],
      "source": [
        "# Install transformers (unless you already installed it for OWL-ViT)\n",
        "# Install fairseq\n",
        "!git clone --branch fixing_prefix_allowed_tokens_fn https://github.com/nicola-decao/fairseq\n",
        "!cd fairseq\n",
        "!pip install --editable ./fairseq/\n",
        "# Install Genre\n",
        "!rm -rf GENRE\n",
        "!git clone https://github.com/facebookresearch/GENRE.git\n",
        "! cd GENRE && pip install ./\n",
        "# Load the model and Trie\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import pickle\n",
        "import re\n",
        "! pip install sentencepiece marisa_trie\n",
        "from genre.trie import MarisaTrie\n",
        "from huggingface_hub import hf_hub_download\n",
        "file_path_marisa_trie = hf_hub_download(\"facebook/mgenre-wiki\", \"titles_lang_all105_marisa_trie_with_redirect.pkl\")\n",
        "with open(file_path_marisa_trie, \"rb\") as f:\n",
        "    trie = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfzr447YUS4w"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mgenre-wiki\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mgenre-wiki\").eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZa7SfMW2e4X"
      },
      "source": [
        "## Analyse subtitles\n",
        "Upload the associated subtitles as .txt. Timecodes need to be encoded in ms. Timecodes need to precede the associated subtitles. Timecode range needs to be separated by exactly one \"-\". \n",
        "In the example, the timecodes do not start at zero, so the initial starting timecode offset has to be subtracted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwYvr1c6TsM0"
      },
      "outputs": [],
      "source": [
        "# Read in all text files\n",
        "import glob\n",
        "path = '/content/*.txt' \n",
        "text_files=glob.glob(path) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfU6LqZCX6Z5"
      },
      "source": [
        "Convert each text file into a dataframe which has the start timecode in row 1, end timecode in row 2 and the subtitle text in row 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlgr9thtUR9n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "time_code_start = []\n",
        "time_code_end = []\n",
        "subtitle_text = []\n",
        "\n",
        "# Loop over the text files in content folder\n",
        "for file in text_files:\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    # Seperate timecodes from subtitle text\n",
        "    for line1,line2 in zip(lines[::2],lines[1::2]):\n",
        "      #Take the first token from line1 as initial time code offset\n",
        "      if len(time_code_start) == 0:\n",
        "        time_code_start.append(line1.split()[0])\n",
        "        time_code_end.append(line1.split()[2])\n",
        "      else:\n",
        "        # For all odd line indeces take the first token, convert string to integer and subtract the offset\n",
        "        time_code_start.append(int(line1.split()[0])-int(time_code_start[0]))\n",
        "        # Take the third token from line1 as end timecode and subtract subtract the offset\n",
        "        time_code_end.append(int(line1.split()[2])-int(time_code_start[0]))\n",
        "      # Add each even line to subtitle text \n",
        "      # remove music from subtitles ()\n",
        "      regex_tc = r\"\\*.*\\*\"\n",
        "      line_2_2 = re.sub(regex_tc, \" \", line2, 0, re.MULTILINE)\n",
        "      # remove all whitespace characters (tab, newline, return, formfeed)   \n",
        "      subtitle_text.append(\" \".join(line_2_2.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bv0PTkThIwy"
      },
      "outputs": [],
      "source": [
        "# Combine time_code_start, end and the associated text in one list of dictionaries \n",
        "text_list_of_dicts = []\n",
        "if len(time_code_start) == len(time_code_end) == len(subtitle_text):\n",
        "  for i in range(len(time_code_start)):\n",
        "    text_list_of_dicts.append({\"1_Start_TimeCode\": time_code_start[i], \"2_End_Timecode\": time_code_end[i], \"3_Text\": subtitle_text[i]})    \n",
        "# Convert to DataFrame\n",
        "text_df = pd.DataFrame(text_list_of_dicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIeIXGXRklXo"
      },
      "outputs": [],
      "source": [
        "# Remove the offset row\n",
        "text_df  = text_df.drop(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSNaCDm7VZs4"
      },
      "outputs": [],
      "source": [
        "# Look at the entire data frame\n",
        "text_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QADE4RdajSbq"
      },
      "source": [
        "Create textblocks of 10 Sentences for analysis with the LandmarkNER. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqJARo_fj17w"
      },
      "outputs": [],
      "source": [
        "# Take all of the subtitle text\n",
        "full_text = ' '.join(subtitle_text)\n",
        "\n",
        "import spacy\n",
        "\n",
        "# My model has no sentencizer component, so I have to use the pretrained spacy model\n",
        "nlp_sent = spacy.load(\"de_core_news_lg\")\n",
        "doc_sent = nlp_sent(full_text)\n",
        "\n",
        "# use the spacy tokenizer to get sentences\n",
        "# Take 10 sentences and append them to subtitle_chunks\n",
        "subtitle_chunks = []\n",
        "chunk_nr = 0\n",
        "chunk_text = \"\" \n",
        "for sent in doc_sent.sents:\n",
        "  chunk_text += (\" \" + sent.text)\n",
        "  chunk_nr += 1\n",
        "  if chunk_nr % 10 == 0:\n",
        "    subtitle_chunks.append(chunk_text)\n",
        "    chunk_text = \"\"\n",
        "    chunk_nr = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXSjGu6rjoib"
      },
      "outputs": [],
      "source": [
        "# Look at the first chunk\n",
        "subtitle_chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMmOgOMB16Q1"
      },
      "source": [
        "Loop through subtitle_chunks, analyze each chunk for landmarks, if there are landmarks, disambiguate them with mGenre and give out the text of the chunk and the detected landmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFhozky3tgzL"
      },
      "outputs": [],
      "source": [
        "# Create an empty list to save the output of the landmarkNER, the landmark titles and the associated chunks to\n",
        "landmark_titles = []\n",
        "\n",
        "for chunk in subtitle_chunks:  \n",
        "    # Create a spacy Doc object\n",
        "    doc = nlp(chunk)\n",
        "    reshaped_text = []\n",
        "    # Reshape detected landmarks for mGenre\n",
        "    for ent in doc.ents:\n",
        "      # Add start and end marker for landmark mention\n",
        "      reshaped_text.append(doc.text.replace(ent.text, '[START]' + ent.text + '[END]'))\n",
        "    generated_text = []\n",
        "    # take the reshaped chunk and create mGenre model output\n",
        "    for sent in reshaped_text:\n",
        "      outputs = model.generate(\n",
        "          **tokenizer(sent, return_tensors=\"pt\"),\n",
        "          num_beams=5,\n",
        "          num_return_sequences=5,\n",
        "          # use constrained beam search to only return valid wikipedia titles\n",
        "          prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n",
        "          )\n",
        "      generated_text.append(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "      # Create regex\n",
        "      re_lang = r\" >> [a-z]+\"\n",
        "    # If a wikipedia title for an entity was detected  \n",
        "    # Loop through the length of generated text / doc.ents / reshaped_text\n",
        "    if generated_text:\n",
        "      for i in range(len(generated_text)):\n",
        "       # Take the prediction with the highest score\n",
        "       gen_text = generated_text[i][0]\n",
        "       # remove irrelevant tokens\n",
        "       shortened_text = re.sub(re_lang, \"\", gen_text, 0, re.MULTILINE)\n",
        "       # Add found entities, their wikipedia title and the current chunk where to landmark_titles\n",
        "       if shortened_text:\n",
        "         landmark_titles.append([doc.ents[i], shortened_text, chunk])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY2L6QKhv8nD"
      },
      "outputs": [],
      "source": [
        "# Convert to dataframe for visual inspection\n",
        "landmark_titles_df = pd.DataFrame(landmark_titles, columns = ['LandmarkNER', 'Disambiguierung', 'Chunk'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO2nORyV_YSa"
      },
      "outputs": [],
      "source": [
        "landmark_titles_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7StvV_-kjpE"
      },
      "source": [
        "Create a list with any detected landmarks and the associated timecodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZaOfdm-kUAA"
      },
      "outputs": [],
      "source": [
        "# This preliminary code checks whether a subtitle line appears in one of the chunks. \n",
        "# If the output of the LandmarkNER also appears in this subtitle line \n",
        "# the corresponding output of mGenre and the start time code and end time code of the subtitle line are \n",
        "# added to the landmark_titles_time_codes list\n",
        "\n",
        "landmark_titles_time_codes = []\n",
        "\n",
        "for i in range(len(landmark_titles)):\n",
        "  for j in range(len(subtitle_text)):\n",
        "    if subtitle_text[j] in landmark_titles[i][2] and landmark_titles[i][0].text in subtitle_text[j]:\n",
        "      landmark_titles_time_codes.append([landmark_titles[i][1], int(time_code_start[j]), int(time_code_end[j])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FNxsigtyjM_"
      },
      "outputs": [],
      "source": [
        "landmark_titles_time_codes[:10] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ5PwiQpqWOh"
      },
      "outputs": [],
      "source": [
        "len(landmark_titles_time_codes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8VJURiSzi39"
      },
      "source": [
        "# Connect building detector and LandmarkNER\n",
        "First, we need to find out whether there are overlapping timecodes at which a building was detect in the video and a landmark was detected in the subtitles.\n",
        "I subtract/add 2000 from the landmark timecodes to account for the fact they may sometimes appear on the screen earlier or later than in the subtitles.\n",
        "\n",
        "The Viam-ID is extracted from the subtitle file. This needs to be named \n",
        "All the images are saved in a folder named \n",
        "In this folder, there is a folder for each landmark title. \n",
        "The filename is the title of the landmark, the Viam-ID of the video and the timecode of the frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhSDQXeR7hGu"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset_lm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjXAWDVwybQe"
      },
      "outputs": [],
      "source": [
        "# Take the file path for the subtitle file and extract the viam-id (everything betweent content/ and a _ or a .)\n",
        "regex = r\"^/content/([^._]*).*\"\n",
        "viam_id = re.search(regex, text_files[0]).group(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7A-P-eZZhry"
      },
      "source": [
        "## Connect subtitles with video frames\n",
        "Include a 2000 ms overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qJsfoKtzKGe"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# # loop through all the time codes at which buildings are detected\n",
        "# for i in range(len(building_video_time_codes)):\n",
        "#   # Loop through all time codes at which landmarks are detected\n",
        "#   for j in range(len(landmark_titles_time_codes)):\n",
        "#     # Check if the current building time code overlaps with a landmark timecode +/- 2000 ms and whether \"/\"\" is in the landmark name\n",
        "#     if  (int(landmark_titles_time_codes[j][1])-2000 <= int(building_video_time_codes[i]) <= int(landmark_titles_time_codes[j][2])+2000) and (\"/\" not in landmark_titles_time_codes[j][0]):\n",
        "#       # check if the folder for the current landmark exists and if not create it\n",
        "#       if not os.path.exists(\"/content/dataset_lm/\" + landmark_titles_time_codes[j][0]):\n",
        "#         os.makedirs(\"/content/dataset_lm/\" + landmark_titles_time_codes[j][0])\n",
        "#       # For overlapping time codes, save the associated frame with the name of the landmark as title.    \n",
        "#       building_frames[i].save(\"/content/dataset_lm/\" + landmark_titles_time_codes[j][0] + \"/\" + viam_id + \"_ \" + landmark_titles_time_codes[j][0]+ \"_\"  + str(building_video_time_codes[i]) + \".jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keMaidILZxPN"
      },
      "source": [
        "## Connect 'Bildinhalt' with video frames\n",
        "no overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_WLJ4dpZ3qn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# loop through all the time codes at which buildings are detected\n",
        "for i in range(len(building_video_time_codes)):\n",
        "  # Loop through all time codes at which landmarks are detected\n",
        "  for j in range(len(landmark_titles_time_codes)):\n",
        "    # Check if the current building time code overlaps with a landmark timecode +/- 2000 ms and whether \"/\"\" is in the landmark name\n",
        "    if  (int(landmark_titles_time_codes[j][1]) <= int(building_video_time_codes[i]) <= int(landmark_titles_time_codes[j][2])) and (\"/\" not in landmark_titles_time_codes[j][0]):\n",
        "      # check if the folder for the current landmark exists and if not create it\n",
        "      if not os.path.exists(\"/content/dataset_lm/\" + landmark_titles_time_codes[j][0]):\n",
        "        os.makedirs(\"/content/dataset_lm/\" + landmark_titles_time_codes[j][0])\n",
        "      # For overlapping time codes, save the associated frame with the name of the landmark as title.    \n",
        "      building_frames[i].save(\"/content/dataset_lm/\" + landmark_titles_time_codes[j][0] + \"/\" + viam_id + \"_ \" + landmark_titles_time_codes[j][0]+ \"_\"  + str(building_video_time_codes[i]) + \".jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dSj12xa8MM1"
      },
      "source": [
        "## Zip the folder with the frames and download it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZvOyb2R8FKV"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/file.zip /content/dataset_lm/\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WicGjY6rw3qv"
      },
      "source": [
        "Currently, you can only analyse one video at a time with this notebook. So to analyse another video you first need to delete the old video and subtitle and then rerun the notebook from \"Preprocess input video\"."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uIcaig48T6yv",
        "SnW9xUhCTHd4",
        "MU_JUEEIg5Xj",
        "dmM3DoCPLppD"
      ],
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 ('.lm_ner_env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "7f9db098c55defaeb2a20ca82dedb9844393a5d5e2bc9ac630bee6686a6a8978"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
